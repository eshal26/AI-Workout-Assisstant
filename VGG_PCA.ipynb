{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLjvt63sHyuHEEXHyI5+TD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshal26/AI-Workout-Assisstant/blob/main/VGG_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import os\n",
        "import shutil\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "L-9ZU3dWjLUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'train_dataset'\n",
        "val_dir = 'validation_dataset'\n",
        "test_dir = 'test_dataset'\n",
        "\n",
        "# Define transformations for training, validation, and testing data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.8238, 0.8539, 0.9391], std=[0.1325, 0.1437, 0.0529])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.8238, 0.8539, 0.9391], std=[0.1325, 0.1437, 0.0529])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "MLiSkobUjS-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class CustomVGGWithPCA(nn.Module):\n",
        "    def __init__(self, num_classes=2, pca_components=50):\n",
        "        super(CustomVGGWithPCA, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 112x112x64\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 56x56x128\n",
        "        )\n",
        "\n",
        "        # Initialize PCA\n",
        "        self.pca = PCA(n_components=pca_components)\n",
        "\n",
        "        # Classifier layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(pca_components, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the features for PCA\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extract_features(x)\n",
        "        device = x.device\n",
        "        # Apply PCA if it is fitted\n",
        "        if hasattr(self.pca, 'components_'):\n",
        "            x = self.pca.transform(x.detach().cpu().numpy())  # Detach and convert to NumPy for PCA\n",
        "            x = torch.from_numpy(x).to(device, dtype=torch.float32)   # Convert back to tensor on the orice\n",
        "        else:\n",
        "            raise RuntimeError(\"PCA must be fitted before the forward pass\")\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "AsdUctVzjUpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "def fit_pca_on_train_data(train_loader, model, batch_size=64):\n",
        "    # Use IncrementalPCA for memory efficiency\n",
        "    ipca = IncrementalPCA(n_components=model.pca.n_components, batch_size=batch_size)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            features = model.extract_features(images)\n",
        "            features = features.detach().cpu().numpy()  # Ensure detach before numpy()\n",
        "            ipca.partial_fit(features)  # Fit incrementally\n",
        "\n",
        "    model.pca = ipca  # Assign the fitted IPCA back to modelâ€™s PCA attribute\n",
        "\n",
        "# Example usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pca_components = 12\n",
        "num_classes = 2\n",
        "\n",
        "model = CustomVGGWithPCA(num_classes=num_classes, pca_components=pca_components).to(device)\n",
        "fit_pca_on_train_data(train_loader, model)\n"
      ],
      "metadata": {
        "id": "q6Gvr93ljXu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = 100 * correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "# Training call example\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQrD5vO7jbAh",
        "outputId": "64e455e1-9ef9-4f87-e31a-1d0943b86be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40: Training Loss: 2.1691, Accuracy: 76.88%\n",
            "Epoch 2/40: Training Loss: 1.8396, Accuracy: 77.04%\n",
            "Epoch 3/40: Training Loss: 2.0479, Accuracy: 76.88%\n",
            "Epoch 4/40: Training Loss: 1.6761, Accuracy: 76.50%\n",
            "Epoch 5/40: Training Loss: 1.6498, Accuracy: 77.04%\n",
            "Epoch 6/40: Training Loss: 1.6048, Accuracy: 75.69%\n",
            "Epoch 7/40: Training Loss: 1.4504, Accuracy: 77.34%\n",
            "Epoch 8/40: Training Loss: 1.2089, Accuracy: 77.65%\n",
            "Epoch 9/40: Training Loss: 1.1386, Accuracy: 77.53%\n",
            "Epoch 10/40: Training Loss: 1.1732, Accuracy: 77.27%\n",
            "Epoch 11/40: Training Loss: 1.0648, Accuracy: 77.73%\n",
            "Epoch 12/40: Training Loss: 1.0000, Accuracy: 77.80%\n",
            "Epoch 13/40: Training Loss: 0.9380, Accuracy: 77.73%\n",
            "Epoch 14/40: Training Loss: 0.8669, Accuracy: 79.07%\n",
            "Epoch 15/40: Training Loss: 0.8874, Accuracy: 77.42%\n",
            "Epoch 16/40: Training Loss: 0.7937, Accuracy: 79.15%\n",
            "Epoch 17/40: Training Loss: 0.8952, Accuracy: 78.26%\n",
            "Epoch 18/40: Training Loss: 0.8247, Accuracy: 77.84%\n",
            "Epoch 19/40: Training Loss: 0.8081, Accuracy: 77.53%\n",
            "Epoch 20/40: Training Loss: 0.7253, Accuracy: 78.69%\n",
            "Epoch 21/40: Training Loss: 0.7248, Accuracy: 78.26%\n",
            "Epoch 22/40: Training Loss: 0.7215, Accuracy: 78.42%\n",
            "Epoch 23/40: Training Loss: 0.6700, Accuracy: 78.92%\n",
            "Epoch 24/40: Training Loss: 0.6848, Accuracy: 79.11%\n",
            "Epoch 25/40: Training Loss: 0.6674, Accuracy: 79.30%\n",
            "Epoch 26/40: Training Loss: 0.6460, Accuracy: 78.76%\n",
            "Epoch 27/40: Training Loss: 0.5874, Accuracy: 80.76%\n",
            "Epoch 28/40: Training Loss: 0.6269, Accuracy: 79.11%\n",
            "Epoch 29/40: Training Loss: 0.5844, Accuracy: 80.61%\n",
            "Epoch 30/40: Training Loss: 0.5835, Accuracy: 79.95%\n",
            "Epoch 31/40: Training Loss: 0.5582, Accuracy: 81.45%\n",
            "Epoch 32/40: Training Loss: 0.5689, Accuracy: 80.68%\n",
            "Epoch 33/40: Training Loss: 0.5775, Accuracy: 79.45%\n",
            "Epoch 34/40: Training Loss: 0.5478, Accuracy: 80.57%\n",
            "Epoch 35/40: Training Loss: 0.5367, Accuracy: 81.11%\n",
            "Epoch 36/40: Training Loss: 0.5321, Accuracy: 81.41%\n",
            "Epoch 37/40: Training Loss: 0.5631, Accuracy: 81.07%\n",
            "Epoch 38/40: Training Loss: 0.5094, Accuracy: 81.87%\n",
            "Epoch 39/40: Training Loss: 0.5142, Accuracy: 81.64%\n",
            "Epoch 40/40: Training Loss: 0.5285, Accuracy: 82.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwkXtNxOjdWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}